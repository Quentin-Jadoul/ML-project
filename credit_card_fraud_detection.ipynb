{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning report - Credit card fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "training_file_path = 'training.csv'\n",
    "X_train = pd.read_csv(training_file_path)\n",
    "\n",
    "test_file_path = 'test.csv'\n",
    "X_test = pd.read_csv(test_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Préparation des données\n",
    "### 1.1 Exploration des données\n",
    "Nous allons commencer par explorer les données afin de voir si elles sont exploitables et si elles nécessitent un traitement particulier.\n",
    "\n",
    "Les points d'attention sont les suivants :\n",
    "- Les données contiennent-elles des valeurs manquantes ?\n",
    "- Les données contiennent-elles des variables catégorielles ?\n",
    "- Les données sont elles équilibrées ?\n",
    "- Les données sont elles au format numérique ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes: TransactionId           0\n",
      "BatchId                 0\n",
      "AccountId               0\n",
      "SubscriptionId          0\n",
      "CustomerId              0\n",
      "CurrencyCode            0\n",
      "CountryCode             0\n",
      "ProviderId              0\n",
      "ProductId               0\n",
      "ProductCategory         0\n",
      "ChannelId               0\n",
      "Amount                  0\n",
      "Value                   0\n",
      "TransactionStartTime    0\n",
      "PricingStrategy         0\n",
      "FraudResult             0\n",
      "dtype: int64\n",
      "Pourcentage de fraude: 0.2%\n",
      "Types:TransactionId            object\n",
      "BatchId                  object\n",
      "AccountId                object\n",
      "SubscriptionId           object\n",
      "CustomerId               object\n",
      "CurrencyCode             object\n",
      "CountryCode               int64\n",
      "ProviderId               object\n",
      "ProductId                object\n",
      "ProductCategory          object\n",
      "ChannelId                object\n",
      "Amount                  float64\n",
      "Value                     int64\n",
      "TransactionStartTime     object\n",
      "PricingStrategy           int64\n",
      "FraudResult               int64\n",
      "dtype: object\n",
      "Valeurs uniques: TransactionId           95662\n",
      "BatchId                 94809\n",
      "AccountId                3633\n",
      "SubscriptionId           3627\n",
      "CustomerId               3742\n",
      "CurrencyCode                1\n",
      "CountryCode                 1\n",
      "ProviderId                  6\n",
      "ProductId                  23\n",
      "ProductCategory             9\n",
      "ChannelId                   4\n",
      "Amount                   1676\n",
      "Value                    1517\n",
      "TransactionStartTime    94556\n",
      "PricingStrategy             4\n",
      "FraudResult                 2\n",
      "dtype: int64\n",
      "Premières lignes:          TransactionId         BatchId       AccountId       SubscriptionId  \\\n",
      "0  TransactionId_76871   BatchId_36123  AccountId_3957   SubscriptionId_887   \n",
      "1  TransactionId_73770   BatchId_15642  AccountId_4841  SubscriptionId_3829   \n",
      "2  TransactionId_26203   BatchId_53941  AccountId_4229   SubscriptionId_222   \n",
      "3    TransactionId_380  BatchId_102363   AccountId_648  SubscriptionId_2185   \n",
      "4  TransactionId_28195   BatchId_38780  AccountId_4841  SubscriptionId_3829   \n",
      "\n",
      "        CustomerId CurrencyCode  CountryCode    ProviderId     ProductId  \\\n",
      "0  CustomerId_4406          UGX          256  ProviderId_6  ProductId_10   \n",
      "1  CustomerId_4406          UGX          256  ProviderId_4   ProductId_6   \n",
      "2  CustomerId_4683          UGX          256  ProviderId_6   ProductId_1   \n",
      "3   CustomerId_988          UGX          256  ProviderId_1  ProductId_21   \n",
      "4   CustomerId_988          UGX          256  ProviderId_4   ProductId_6   \n",
      "\n",
      "      ProductCategory    ChannelId   Amount  Value  TransactionStartTime  \\\n",
      "0             airtime  ChannelId_3   1000.0   1000  2018-11-15T02:18:49Z   \n",
      "1  financial_services  ChannelId_2    -20.0     20  2018-11-15T02:19:08Z   \n",
      "2             airtime  ChannelId_3    500.0    500  2018-11-15T02:44:21Z   \n",
      "3        utility_bill  ChannelId_3  20000.0  21800  2018-11-15T03:32:55Z   \n",
      "4  financial_services  ChannelId_2   -644.0    644  2018-11-15T03:34:21Z   \n",
      "\n",
      "   PricingStrategy  FraudResult  \n",
      "0                2            0  \n",
      "1                2            0  \n",
      "2                2            0  \n",
      "3                2            0  \n",
      "4                2            0  \n",
      "Statistiques descriptives:        CountryCode        Amount         Value  PricingStrategy   FraudResult\n",
      "count      95662.0  9.566200e+04  9.566200e+04     95662.000000  95662.000000\n",
      "mean         256.0  6.717846e+03  9.900584e+03         2.255974      0.002018\n",
      "std            0.0  1.233068e+05  1.231221e+05         0.732924      0.044872\n",
      "min          256.0 -1.000000e+06  2.000000e+00         0.000000      0.000000\n",
      "25%          256.0 -5.000000e+01  2.750000e+02         2.000000      0.000000\n",
      "50%          256.0  1.000000e+03  1.000000e+03         2.000000      0.000000\n",
      "75%          256.0  2.800000e+03  5.000000e+03         2.000000      0.000000\n",
      "max          256.0  9.880000e+06  9.880000e+06         4.000000      1.000000\n"
     ]
    }
   ],
   "source": [
    "# On print le nombre de valeurs manquantes par colonne\n",
    "print(f\"Valeurs manquantes: {X_train.isnull().sum()}\")\n",
    "\n",
    "# On mesure l'équilibre des classes\n",
    "X_train_no_fraud = X_train[X_train['FraudResult']==0]\n",
    "X_train_fraud = X_train[X_train['FraudResult']==1]\n",
    "\n",
    "percentage_minority = len(X_train_fraud)/(len(X_train_no_fraud) + len(X_train_fraud)) * 100\n",
    "print(f\"Pourcentage de fraude: {round(percentage_minority, 2)}%\")\n",
    "\n",
    "# On print le type de chaque colonne\n",
    "print(f\"Types:{X_train.dtypes}\")\n",
    "\n",
    "# On print le nombre de valeurs uniques par colonne\n",
    "print(f\"Valeurs uniques: {X_train.nunique()}\")\n",
    "\n",
    "# On print les premières lignes du dataset\n",
    "print(f\"Premières lignes: {X_train.head()}\")\n",
    "\n",
    "# On print les statistiques descriptives du dataset\n",
    "print(f\"Statistiques descriptives: {X_train.describe()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les observations que nous avons faites sur les données :\n",
    "- Les données ne contiennent pas de valeurs manquantes\n",
    "- Les données sont fort déséquilibrées (0,2% de fraudes)\n",
    "- Des données qui devraient être numériques sont au format object\n",
    "- **CurrencyCode** et **CountryCode** ne sont pas exploitables car elles ne contienent qu'une seule valeur\n",
    "- **ProviderId**, **ProductId**, **ProductCategory**, **ChannelId** et **PricingStrategy** sont des variables catégorielles\n",
    "\n",
    "### 1.2 Rééquilibrage des données\n",
    "Face a un jeu de données en déséquilibre, deux solutions s'offrent à nous:\n",
    "- Effectuer un oversampling de la classe minoritaire (fraud) en créant des observations de cette classe.\n",
    "- Effectuer un undersampling de la classe majoritaire (normal) en supprimant des observations de cette classe.\n",
    "\n",
    "Nous allons ici choisir la première solution, en utilisant la methode resample de la librairie sklearn. Celle ci va nous permettre de créer un dataset équilibré en augmentant le nombre d'observations de la classe minoritaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fraud_upsampled = resample(X_train_fraud, replace=True, n_samples=len(X_train_no_fraud), random_state=123)\n",
    "\n",
    "X_train_upsampled = pd.concat([X_train_no_fraud, X_train_fraud])\n",
    "\n",
    "y = X_train_upsampled['FraudResult']\n",
    "# On supprime la colonne FraudResult du dataset qui ne nous est désormais plus utile\n",
    "X_train_upsampled = X_train_upsampled.drop(['FraudResult'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Selection des variables\n",
    "Nous allons ici sélectionner les variables qui nous semblent pertinentes pour la prédiction. Nous allons donc retirer les variables suivantes :\n",
    "- **CurrencyCode** et **CountryCode** car elles ne contiennent qu'une seule valeur\n",
    "- **BatchId** car il s'agit de l'identifiant d'un groupe de transactions, non disponible au moment de la transaction et a priori pas lié à la fraude\n",
    "- **TransactionId** car il s'agit d'un identifiant unique, a priori non lié à la fraude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime les colonnes suivantes\n",
    "X_train_upsampled = X_train_upsampled.drop(['TransactionId', 'BatchId', 'CurrencyCode', 'CountryCode', 'TransactionStartTime'], axis=1)\n",
    "# On stock les index (TransactionId) des lignes du dataset de test avant de supprimer la colonne TransactionId\n",
    "X_test_index = X_test['TransactionId']\n",
    "X_test = X_test.drop(['TransactionId', 'BatchId', 'CurrencyCode', 'CountryCode', 'TransactionStartTime'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Conversion des variables numériques\n",
    "Nous allons convertir les variables numériques au format object en variables numériques au format int. Pour cela nous retirons la partie texte des valeurs et nous convertissons le résultat en int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les colonnes qui contiennent Id dans leur nom\n",
    "id_columns = [col for col in X_train_upsampled.columns if 'Id' in col]\n",
    "\n",
    "# On définit une fonction qui va enlever la partie textuelle des identifiants\n",
    "def transform_id(feature):\n",
    "    if isinstance(feature, str):\n",
    "        return feature.split(\"_\")[-1]\n",
    "    else:\n",
    "        return feature\n",
    "    \n",
    "# On applique la fonction sur les colonnes d'identifiants\n",
    "for column in id_columns:\n",
    "    X_train_upsampled[column] = X_train_upsampled[column].apply(transform_id)\n",
    "    X_test[column] = X_test[column].apply(transform_id)\n",
    "\n",
    "# On transforme les colonnes d'identifiants en int\n",
    "X_train_upsampled[id_columns] = X_train_upsampled[id_columns].astype('int64')\n",
    "X_test[id_columns] = X_test[id_columns].astype('int64')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Encodage des variables catégorielles\n",
    "Nous allons encoder les variables catégorielles de deux manières différentes :\n",
    "- **One hot encoding** pour les variables contenants moins de 10 modalités\n",
    "- **Target encoding** pour les variables contenant plus de 10 modalités\n",
    "\n",
    "On fera donc du **one hot encoding** sur les variables **ProviderId**, **ProductCategory**, **ChannelId** et **PricingStrategy**.\n",
    "**ProductId** contient 23 modalités, nous allons donc faire du **target encoding** sur cette variable. Il est cependant déjà au format int, nous n'avons donc pas besoin de le convertir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AccountId  SubscriptionId  CustomerId  ProductId   Amount  Value    0    1  \\\n",
      "0       3957             887        4406         10   1000.0   1000  0.0  0.0   \n",
      "1       4841            3829        4406          6    -20.0     20  0.0  0.0   \n",
      "2       4229             222        4683          1    500.0    500  0.0  0.0   \n",
      "3        648            2185         988         21  20000.0  21800  1.0  0.0   \n",
      "4       4841            3829         988          6   -644.0    644  0.0  0.0   \n",
      "\n",
      "     2    3  ...   13   14   15   16   17   18   19   20   21   22  \n",
      "0  0.0  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1  0.0  1.0  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "2  0.0  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  \n",
      "3  0.0  0.0  ...  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  \n",
      "4  0.0  1.0  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qjado\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# On liste les colonnes catégorielles\n",
    "categorical_columns = ['ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'PricingStrategy']\n",
    "# On ne garde que celles contenants moins de 10 modalités\n",
    "categorical_columns = [col for col in categorical_columns if X_train_upsampled[col].nunique() < 10]\n",
    "\n",
    "# On effectue un one hot encoding sur les colonnes catégorielles\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train_upsampled[categorical_columns]))\n",
    "OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[categorical_columns]))\n",
    "\n",
    "# On remet les index\n",
    "OH_cols_train.index = X_train_upsampled.index\n",
    "OH_cols_test.index = X_test.index\n",
    "\n",
    "# On enlève les colonnes catégorielles\n",
    "num_X_train = X_train_upsampled.drop(categorical_columns, axis=1)\n",
    "num_X_test = X_test.drop(categorical_columns, axis=1)\n",
    "\n",
    "# On ajoute les colonnes encodées\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\n",
    "\n",
    "# On transforme les noms de colonnes en string\n",
    "OH_X_train.columns = OH_X_train.columns.map(str)\n",
    "OH_X_test.columns = OH_X_test.columns.map(str)\n",
    "\n",
    "# On print les premières lignes du dataset\n",
    "print(OH_X_train.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modélisation\n",
    "### 2.1. Séparation des données\n",
    "Nous allons ici séparer nos données en deux jeux de données:\n",
    "- Un jeu de données d'entrainement (80% des données)\n",
    "- Un jeu de données de test (20% des données)\n",
    "Ceci nous permettra de tester la performance de notre modèle sur des données qu'il n'a jamais vu.\n",
    "### 2.2 Choix du modèle\n",
    "Nous allons ici utiliser un modèle de type **Random Forest**. Ce modèle est un modèle d'apprentissage supervisé qui peut être utilisé pour la classification ou la régression. Il s'agit d'un modèle très utilisé en machine learning car il est très performant et qu'il permet de traiter des données manquantes ou des données non équilibrées.\n",
    "### 2.3. Entrainement du modèle\n",
    "Nous allons ici entrainer notre modèle sur notre jeu de données d'entrainement. Nous allons ensuite tester la performance de notre modèle sur notre jeu de données de test.\n",
    "### 2.4. Evaluation du modèle\n",
    "Nous allons ici évaluer la performance de notre modèle en utilisant la matrice de confusion. Cette matrice nous permet de visualiser les prédictions de notre modèle. Elle nous permet de voir les vrais positifs, les vrais négatifs, les faux positifs et les faux négatifs. Nous allons également calculer le score de précision de notre modèle. Ce score nous permet de voir la proportion de prédictions correctes effectuées par notre modèle.\n",
    "Nous calculons également le score f1 de notre modèle. Ce score est la moyenne harmonique entre la précision et le rappel. Il permet de mesurer la performance d'un modèle de classification binaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qjado\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8717948717948718\n",
      "[[19089     2]\n",
      " [    8    34]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-22 {color: black;background-color: white;}#sk-container-id-22 pre{padding: 0;}#sk-container-id-22 div.sk-toggleable {background-color: white;}#sk-container-id-22 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-22 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-22 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-22 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-22 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-22 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-22 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-22 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-22 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-22 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-22 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-22 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-22 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-22 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-22 div.sk-item {position: relative;z-index: 1;}#sk-container-id-22 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-22 div.sk-item::before, #sk-container-id-22 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-22 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-22 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-22 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-22 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-22 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-22 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-22 div.sk-label-container {text-align: center;}#sk-container-id-22 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-22 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-22\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=2, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=2, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=0, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=2, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=0, ...)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Séparation des données en train et validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(OH_X_train, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# On définit le modèle\n",
    "card_fraud_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "card_fraud_model_XGB = XGBClassifier(n_estimators=2, random_state=0, learning_rate=0.1, max_depth=2, use_label_encoder=False)\n",
    "\n",
    "# On entraine le modèle\n",
    "card_fraud_model.fit(X_train, y_train)\n",
    "card_fraud_model_XGB.fit(X_train, y_train)\n",
    "\n",
    "# On fait des prédictions sur le dataset de validation\n",
    "predictions = card_fraud_model.predict(X_valid)\n",
    "\n",
    "# On evalue le modèle avec la métrique F1 et la matrice de confusion\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "print(f1_score(y_valid, predictions))\n",
    "print(confusion_matrix(y_valid, predictions))\n",
    "\n",
    "# # On optimise les hyperparamètres du modèle XGBClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [2, 4, 6, 8, 10],\n",
    "#     'learning_rate': [0.1, 0.01, 0.05],\n",
    "#     'max_depth': [2, 4, 6, 8, 10]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=card_fraud_model_XGB, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_)\n",
    "# print(grid_search.best_estimator_)\n",
    "# print(grid_search.best_index_)\n",
    "# print(grid_search.scorer_)\n",
    "# print(grid_search.n_splits_)\n",
    "# print(grid_search.refit_time_)\n",
    "# print(grid_search.cv_results_)\n",
    "# print(grid_search.predict(X_valid))\n",
    "\n",
    "\n",
    "# on fit les modeles sur l'entiereté du training set\n",
    "card_fraud_model.fit(OH_X_train, y)\n",
    "card_fraud_model_XGB.fit(OH_X_train, y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "         TransactionId  FraudResult\n",
      "0  TransactionId_50600            0\n",
      "1  TransactionId_95109            0\n",
      "2  TransactionId_47357            0\n",
      "3  TransactionId_28185            0\n",
      "4  TransactionId_22140            0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionId</th>\n",
       "      <th>FraudResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TransactionId_50600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TransactionId_95109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TransactionId_47357</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TransactionId_28185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TransactionId_22140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TransactionId  FraudResult\n",
       "0  TransactionId_50600            0\n",
       "1  TransactionId_95109            0\n",
       "2  TransactionId_47357            0\n",
       "3  TransactionId_28185            0\n",
       "4  TransactionId_22140            0"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we make predictions which we will submit to Zindi, it need to be submit as a csv file containing the TransactionId and FraudResult\n",
    "# The Transaction id is writen as \"TransactionId_XXXX\" so we need to split it and take the last element\n",
    "predictions = card_fraud_model_XGB.predict(OH_X_test)\n",
    "# we print the first 5 predictions\n",
    "print(predictions[:5])\n",
    "\n",
    "# On réintègre la colonne TransactionId a partir de X_test_index\n",
    "OH_X_test['TransactionId'] = X_test_index\n",
    "\n",
    "# We create a dataframe containing the TransactionId and the predictions\n",
    "submission = pd.DataFrame({'TransactionId': OH_X_test.TransactionId, 'FraudResult': predictions})\n",
    "print(submission.head())\n",
    "\n",
    "# We add 'TransactionId_' to the TransactionId column\n",
    "submission['TransactionId'] = submission['TransactionId'].astype(str)\n",
    "\n",
    "# We save the dataframe as a csv file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
