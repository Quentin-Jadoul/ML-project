{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning report - Credit card fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "training_file_path = 'training.csv'\n",
    "X_train = pd.read_csv(training_file_path)\n",
    "\n",
    "test_file_path = 'test.csv'\n",
    "X_test = pd.read_csv(test_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Manipulation des données\n",
    "### 1.1. Données déséquilibrées\n",
    "Nous sommes ici face a un dataset en fort déséquilibre. En effet, le nombre de transactions frauduleuses est très faible par rapport au nombre de transactions normales (~0.2%). Nous allons donc devoir faire attention à cela lors de la modélisation.\n",
    "\n",
    "Deux solutions s'offrent à nous:\n",
    "- Effectuer un oversampling de la classe minoritaire (fraud) afin d'obtenir un dataset équilibré\n",
    "- Effectuer un undersampling de la classe majoritaire (normal) afin d'obtenir un dataset équilibré\n",
    "\n",
    "Nous allons ici choisir la première solution, en utilisant la methode resample de la librairie sklearn. Celle ci va nous permettre de créer un dataset équilibré en augmentant le nombre d'observations de la classe minoritaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud percantage before upsampling: 0.2%\n"
     ]
    }
   ],
   "source": [
    "X_train_no_fraud = X_train[X_train['FraudResult']==0]\n",
    "X_train_fraud = X_train[X_train['FraudResult']==1]\n",
    "\n",
    "# We calculate the percentage of the minority class\n",
    "percentage_minority = len(X_train_fraud)/(len(X_train_no_fraud) + len(X_train_fraud)) * 100\n",
    "print(f\"Fraud percantage before upsampling: {round(percentage_minority, 2)}%\")\n",
    "\n",
    "X_train_fraud_upsampled = resample(X_train_fraud, replace=True, n_samples=len(X_train_no_fraud), random_state=123)\n",
    "\n",
    "X_train_upsampled = pd.concat([X_train_no_fraud, X_train_fraud])\n",
    "\n",
    "y = X_train_upsampled['FraudResult']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Premier tri et formatage des données\n",
    "Nous allons ici effectuer un premier tri sur nos données. En effet, nous n'allons pas conserver les colonnes qui ne nous semblent pas pertinentes pour notre modele. Nous allons également formater les colonnes contenants des chaines de caractères afin de les rendre exploitables.\n",
    "\n",
    "Nous eliminerons donc les colonnes suivantes:\n",
    "- **BatchId**: Il s'agit de l'identifiant du lot auquel appartient la transaction. Cet identifiant ne nous semble pas pertinent pour détecter une fraude.\n",
    "- **CurrencyCode**: Il s'agit du code de la devise utilisée pour la transaction. Cet identifiant ne nous est pas utile car nous avons ici affaire à une seule devise (UGX).\n",
    "- **CountryCode**: Il s'agit du code du pays dans lequel la transaction a été effectuée. Cet identifiant ne nous est pas utile car nous avons ici affaire à un seul pays (256).\n",
    "- **TransactionStartTime**: Il s'agit de la date et de l'heure de la transaction.\n",
    "\n",
    "Nous retirons ensuite la partie textuelle des différents identifiants afin de ne garder que la partie numérique. Nous effectuons également un one hot encoding sur la colonne **ProductCategory** afin de la rendre exploitable par notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UGX']\n",
      "TransactionId        int64\n",
      "AccountId            int64\n",
      "SubscriptionId       int64\n",
      "CustomerId           int64\n",
      "ProviderId           int64\n",
      "ProductId            int64\n",
      "ChannelId            int64\n",
      "Amount             float64\n",
      "Value                int64\n",
      "PricingStrategy      int64\n",
      "ProductCategory     object\n",
      "dtype: object\n",
      "   TransactionId  AccountId  SubscriptionId  CustomerId  ProviderId  \\\n",
      "0          76871       3957             887        4406           6   \n",
      "1          73770       4841            3829        4406           4   \n",
      "2          26203       4229             222        4683           6   \n",
      "3            380        648            2185         988           1   \n",
      "4          28195       4841            3829         988           4   \n",
      "\n",
      "   ProductId  ChannelId   Amount  Value  PricingStrategy    0    1    2    3  \\\n",
      "0         10          3   1000.0   1000                2  1.0  0.0  0.0  0.0   \n",
      "1          6          2    -20.0     20                2  0.0  0.0  1.0  0.0   \n",
      "2          1          3    500.0    500                2  1.0  0.0  0.0  0.0   \n",
      "3         21          3  20000.0  21800                2  0.0  0.0  0.0  0.0   \n",
      "4          6          2   -644.0    644                2  0.0  0.0  1.0  0.0   \n",
      "\n",
      "     4    5    6    7    8  \n",
      "0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  1.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qjado\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# On print ici les valeurs unique de la variable currency code pour voir si il est pertinent de la garder.\n",
    "print(X_train_upsampled['CurrencyCode'].unique())\n",
    "\n",
    "# On indique ici les features que l'on veut garder\n",
    "features = ['TransactionId', 'AccountId', 'SubscriptionId', 'CustomerId', 'ProviderId', 'ProductId', 'ChannelId', 'Amount', 'Value', 'PricingStrategy', 'ProductCategory']\n",
    "\n",
    "# On applique ici la sélection des features\n",
    "X_train_upsampled = X_train_upsampled[features]\n",
    "X_test = X_test[features]\n",
    "\n",
    "# On liste les collones d'identifiants auquel on veut enlever la partie textuelle\n",
    "id_columns = ['TransactionId', 'AccountId', 'SubscriptionId', 'CustomerId', 'ProviderId', 'ProductId', 'ChannelId']\n",
    "\n",
    "# On définit une fonction qui va enlever la partie textuelle des identifiants\n",
    "def transform_id(feature):\n",
    "    if isinstance(feature, str):\n",
    "        return feature.split(\"_\")[-1]\n",
    "    else:\n",
    "        return feature\n",
    "    \n",
    "# On applique la fonction sur les colonnes d'identifiants\n",
    "for column in id_columns:\n",
    "    X_train_upsampled[column] = X_train_upsampled[column].apply(transform_id)\n",
    "    X_test[column] = X_test[column].apply(transform_id)\n",
    "\n",
    "# On transforme les colonnes d'identifiants en int\n",
    "X_train_upsampled[id_columns] = X_train_upsampled[id_columns].astype('int64')\n",
    "X_test[id_columns] = X_test[id_columns].astype('int64')\n",
    "\n",
    "# on print le type des colonnes pour vérifier que tout est ok\n",
    "print(X_train_upsampled.dtypes)\n",
    "\n",
    "# On applique du one hot encoding sur ProductCategory\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train_upsampled[['ProductCategory']]))\n",
    "OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[['ProductCategory']]))\n",
    "\n",
    "# On remet les index\n",
    "OH_cols_train.index = X_train_upsampled.index\n",
    "OH_cols_test.index = X_test.index\n",
    "\n",
    "# On enlève la colonne ProductCategory\n",
    "num_X_train = X_train_upsampled.drop('ProductCategory', axis=1)\n",
    "num_X_test = X_test.drop('ProductCategory', axis=1)\n",
    "\n",
    "# On ajoute les colonnes encodées\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\n",
    "\n",
    "# On transforme les noms de colonnes en string\n",
    "OH_X_train.columns = OH_X_train.columns.map(str)\n",
    "OH_X_test.columns = OH_X_test.columns.map(str)\n",
    "\n",
    "# On print les premières lignes du dataset pour voir si les identifiants ont bien été transformés\n",
    "print(OH_X_train.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modélisation\n",
    "### 2.1 Choix du modèle\n",
    "Nous allons ici utiliser un modèle de type **Random Forest**. Ce modèle est un modèle d'apprentissage supervisé qui peut être utilisé pour la classification ou la régression. Il s'agit d'un modèle très utilisé en machine learning car il est très performant et qu'il permet de traiter des données manquantes ou des données non équilibrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_fraud_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "\n",
    "card_fraud_model.fit(OH_X_train, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "   TransactionId  FraudResult\n",
      "0          50600            0\n",
      "1          95109            0\n",
      "2          47357            0\n",
      "3          28185            0\n",
      "4          22140            0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionId</th>\n",
       "      <th>FraudResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TransactionId_50600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TransactionId_95109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TransactionId_47357</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TransactionId_28185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TransactionId_22140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TransactionId  FraudResult\n",
       "0  TransactionId_50600            0\n",
       "1  TransactionId_95109            0\n",
       "2  TransactionId_47357            0\n",
       "3  TransactionId_28185            0\n",
       "4  TransactionId_22140            0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we make predictions which we will submit to Zindi, it need to be submit as a csv file containing the TransactionId and FraudResult\n",
    "# The Transaction id is writen as \"TransactionId_XXXX\" so we need to split it and take the last element\n",
    "predictions = card_fraud_model.predict(OH_X_test)\n",
    "# we print the first 5 predictions\n",
    "print(predictions[:5])\n",
    "\n",
    "# We create a dataframe containing the TransactionId and the predictions\n",
    "submission = pd.DataFrame({'TransactionId': OH_X_test.TransactionId, 'FraudResult': predictions})\n",
    "print(submission.head())\n",
    "\n",
    "# We add 'TransactionId_' to the TransactionId column\n",
    "submission['TransactionId'] = 'TransactionId_' + submission['TransactionId'].astype(str)\n",
    "\n",
    "# We save the dataframe as a csv file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
